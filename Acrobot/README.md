# ü§ñ Acrobot: Treinamento e Visualiza√ß√£o

Este diret√≥rio cont√©m os scripts e resultados para o treinamento e compara√ß√£o de algoritmos de Aprendizado por Refor√ßo no ambiente **Acrobot-v1**.

## üìö Sobre o Ambiente Acrobot-v1

O Acrobot √© um problema cl√°ssico de controle onde um p√™ndulo duplo √© preso a um ponto fixo. O objetivo √© balan√ßar o p√™ndulo at√© que a ponta atinja uma altura alvo, usando apenas a for√ßa aplicada na junta do meio.

### Caracter√≠sticas do Ambiente:
- **Estado**: 6 vari√°veis cont√≠nuas
  - cos(Œ∏‚ÇÅ) e sin(Œ∏‚ÇÅ): Cosseno e seno do √¢ngulo do primeiro link
  - cos(Œ∏‚ÇÇ) e sin(Œ∏‚ÇÇ): Cosseno e seno do √¢ngulo do segundo link
  - Œ∏‚ÇÅÃá: Velocidade angular do primeiro link
  - Œ∏‚ÇÇÃá: Velocidade angular do segundo link
- **A√ß√µes**: 3 a√ß√µes discretas
  - -1: Torque negativo
  - 0: Sem torque
  - +1: Torque positivo
- **Recompensa**: 
  - -1 para cada passo
  - +1 quando a ponta atinge a altura alvo
- **Epis√≥dio Termina**: 
  - Quando a ponta atinge a altura alvo
  - Ap√≥s 500 passos

## üìÅ Estrutura de Arquivos

### Scripts de Treinamento
- `train_dqn_acro.py`: Implementa√ß√£o do Deep Q-Network (DQN)
- `train_a2c_acro.py`: Implementa√ß√£o do Advantage Actor-Critic (A2C)
- `train_ppo_acro.py`: Implementa√ß√£o do Proximal Policy Optimization (PPO)

### Scripts de An√°lise
- `comparador.py`: Compara√ß√£o de desempenho entre os algoritmos
- `visualizar.py`: Visualiza√ß√£o interativa dos modelos treinados

### Diret√≥rios de Dados
- `models/`: Armazena os modelos treinados e checkpoints
  - `dqn_acro_models/`: Modelos DQN
  - `a2c_acro_models/`: Modelos A2C
  - `ppo_acro_models/`: Modelos PPO
- `logs/`: Logs de treinamento para visualiza√ß√£o no TensorBoard
- `resultados/`: Resultados da compara√ß√£o (tabelas e gr√°ficos)

## üöÄ Como Usar

### 1. Treinamento dos Modelos

Para treinar cada algoritmo:
```bash
# Treinar DQN
python train_dqn_acro.py

# Treinar A2C
python train_a2c_acro.py

# Treinar PPO
python train_ppo_acro.py
```

### 2. Compara√ß√£o dos Modelos

Para comparar o desempenho dos algoritmos:
```bash
python comparador.py
```

### 3. Visualiza√ß√£o dos Modelos

Para visualizar um modelo treinado:
```bash
python visualizar.py
```
Modifique a vari√°vel `ALGO` no in√≠cio do arquivo para escolher entre "DQN", "A2C" ou "PPO".

## üìä Resultados da Compara√ß√£o

### Gr√°ficos de Desempenho

![Recompensa M√©dia](resultados/recompensa_media.png)
*Evolu√ß√£o da recompensa m√©dia ao longo do treinamento*

![Estabilidade da Recompensa](resultados/estabilidade_recompensa.png)
*Desvio padr√£o das recompensas ao longo do treinamento*

![Tempo de Avalia√ß√£o](resultados/tempo_avaliacao.png)
*Tempo necess√°rio para avaliar cada modelo*

### Tabela Comparativa

| Algoritmo | Recompensa M√©dia | Desvio Padr√£o | Tempo de Avalia√ß√£o (s) |
|-----------|------------------|---------------|------------------------|
| DQN       | -85.3           | 35.2          | 0.16                  |
| A2C       | -82.7           | 28.6          | 0.13                  |
| PPO       | -78.5           | 22.1          | 0.15                  |

### Converg√™ncia
- **DQN**: 500k passos
- **A2C**: 300k passos
- **PPO**: 200k passos

## üèÜ Resultados Finais

### Compara√ß√£o dos Algoritmos

| Algoritmo | Recompensa M√©dia | Desvio Padr√£o | Tempo de Avalia√ß√£o | Converg√™ncia |
|-----------|------------------|---------------|-------------------|--------------|
| PPO       | -78.5           | 22.1          | 0.15s            | 200k        |
| A2C       | -82.7           | 28.6          | 0.13s            | 300k        |
| DQN       | -85.3           | 35.2          | 0.16s            | 500k        |

### An√°lise dos Resultados

O PPO (Proximal Policy Optimization) demonstrou ser o algoritmo mais eficiente para o ambiente Acrobot, alcan√ßando a melhor recompensa m√©dia (-78.5) e maior estabilidade (desvio padr√£o de 22.1). Sua converg√™ncia em 200k passos, combinada com um tempo de avalia√ß√£o competitivo de 0.15s, mostra um excelente equil√≠brio entre desempenho e efici√™ncia.

O A2C (Advantage Actor-Critic) apresentou um desempenho intermedi√°rio, com uma recompensa m√©dia de -82.7 e desvio padr√£o de 28.6. Sua principal vantagem foi a converg√™ncia mais r√°pida (300k passos) e o menor tempo de avalia√ß√£o (0.13s).

O DQN (Deep Q-Network) teve o desempenho mais modesto, com recompensa m√©dia de -85.3 e maior variabilidade (desvio padr√£o de 35.2). Sua converg√™ncia mais lenta (500k passos) e maior tempo de avalia√ß√£o (0.16s) indicam que pode n√£o ser a melhor escolha para este ambiente espec√≠fico.

### Compara√ß√£o com Outros Ambientes

| Caracter√≠stica | Acrobot | CartPole | LunarLander |
|----------------|---------|----------|-------------|
| Espa√ßo de Estados | 6 | 4 | 8 |
| Espa√ßo de A√ß√µes | 3 | 2 | 4 |
| Recompensa M√°xima | -100 | 500 | 200 |
| Complexidade | M√©dia | Baixa | M√©dia |
| Tempo de Treinamento | M√©dio | Menor | Maior |
| Estabilidade | M√©dia | Maior | Menor |
| Objetivo | Balan√ßar | Equilibrar | Pousar |
| Tipo de Recompensa | Negativa | Positiva | Mista |

### An√°lise Comparativa

1. **Complexidade do Ambiente**
   - Acrobot: Mais complexo que CartPole, menos que LunarLander
   - Estados: Combina√ß√£o de √¢ngulos e velocidades
   - A√ß√µes: Tr√™s n√≠veis de torque
   - Recompensa: Penaliza√ß√£o por tempo

2. **Desafios Espec√≠ficos**
   - Necessidade de coordena√ß√£o entre os dois links
   - Ac√∫mulo de momentum
   - Controle preciso do torque
   - Explora√ß√£o eficiente do espa√ßo de estados

3. **Ajustes Espec√≠ficos**
   - Learning rates mais conservadores
   - Maior √™nfase na explora√ß√£o
   - Buffer de replay maior para DQN
   - Mais √©pocas de treinamento para PPO

4. **Resultados Finais**
   - PPO: Melhor desempenho (-78.5) e maior estabilidade (22.1)
   - A2C: Desempenho intermedi√°rio (-82.7) e boa estabilidade (28.6)
   - DQN: Desempenho mais baixo (-85.3) e menor estabilidade (35.2)
   - Todos os algoritmos convergiram antes dos 1M passos
   - Tempos de avalia√ß√£o similares (0.13-0.16s)

## üîç Explica√ß√£o Detalhada do C√≥digo

### 1. Configura√ß√£o do Ambiente

```python
env = gym.make("Acrobot-v1")
```
- **Por que n√£o usar normaliza√ß√£o?** 
  - O Acrobot tem um espa√ßo de estados bem definido
  - Os valores das observa√ß√µes j√° est√£o em escalas razo√°veis
  - A normaliza√ß√£o poderia adicionar complexidade desnecess√°ria

### 2. Configura√ß√£o do PPO

```python
model = PPO(
    "MlpPolicy",
    env,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    ent_coef=0.01
)
```

**Explica√ß√£o dos hiperpar√¢metros:**
- `learning_rate=3e-4`: Taxa de aprendizado moderada para garantir converg√™ncia est√°vel
- `n_steps=2048`: N√∫mero de passos por atualiza√ß√£o, balanceando efici√™ncia e estabilidade
- `batch_size=64`: Tamanho do batch otimizado para mem√≥ria e converg√™ncia
- `n_epochs=10`: N√∫mero de √©pocas de treinamento por atualiza√ß√£o
- `gamma=0.99`: Fator de desconto alto para valorizar recompensas futuras
- `gae_lambda=0.95`: Par√¢metro GAE para reduzir vari√¢ncia nas estimativas de vantagem
- `clip_range=0.2`: Limite de atualiza√ß√£o da pol√≠tica para evitar mudan√ßas bruscas
- `ent_coef=0.01`: Coeficiente de entropia para manter explora√ß√£o moderada

### 3. Configura√ß√£o do DQN

```python
model = DQN(
    "MlpPolicy",
    env,
    learning_rate=1e-3,
    buffer_size=50000,
    learning_starts=10000,
    batch_size=64,
    tau=1.0,
    gamma=0.99,
    train_freq=4,
    gradient_steps=1,
    target_update_interval=1000,
    exploration_fraction=0.1,
    exploration_initial_eps=1.0,
    exploration_final_eps=0.05,
    max_grad_norm=10
)
```

**Explica√ß√£o dos hiperpar√¢metros:**
- `learning_rate=1e-3`: Taxa de aprendizado mais alta que PPO devido √† natureza do DQN
- `buffer_size=50000`: Tamanho do buffer de replay para armazenar experi√™ncias
- `learning_starts=10000`: N√∫mero de passos antes de come√ßar o treinamento
- `batch_size=64`: Tamanho do batch para atualiza√ß√µes da rede
- `tau=1.0`: Taxa de atualiza√ß√£o da rede alvo (1.0 = atualiza√ß√£o completa)
- `train_freq=4`: Frequ√™ncia de treinamento em rela√ß√£o aos passos do ambiente
- `gradient_steps=1`: N√∫mero de passos de gradiente por atualiza√ß√£o
- `target_update_interval=1000`: Frequ√™ncia de atualiza√ß√£o da rede alvo
- `exploration_fraction=0.1`: Fra√ß√£o do treinamento dedicada √† explora√ß√£o
- `exploration_initial_eps=1.0`: Taxa de explora√ß√£o inicial (100%)
- `exploration_final_eps=0.05`: Taxa de explora√ß√£o final (5%)
- `max_grad_norm=10`: Limite para clipping do gradiente

### 4. Configura√ß√£o do A2C

```python
model = A2C(
    "MlpPolicy",
    env,
    learning_rate=7e-4,
    n_steps=5,
    gamma=0.99,
    ent_coef=0.01,
    vf_coef=0.5,
    max_grad_norm=0.5,
    rms_prop_eps=1e-5,
    use_rms_prop=True,
    use_sde=False,
    sde_sample_freq=-1,
    normalize_advantage=False
)
```

**Explica√ß√£o dos hiperpar√¢metros:**
- `learning_rate=7e-4`: Taxa de aprendizado espec√≠fica para A2C
- `n_steps=5`: N√∫mero de passos por atualiza√ß√£o, menor que PPO
- `ent_coef=0.01`: Coeficiente de entropia para explora√ß√£o
- `vf_coef=0.5`: Peso da fun√ß√£o de valor na fun√ß√£o de perda
- `max_grad_norm=0.5`: Limite para clipping do gradiente
- `rms_prop_eps=1e-5`: Epsilon para o otimizador RMSprop
- `use_rms_prop=True`: Usar RMSprop como otimizador
- `use_sde=False`: N√£o usar State Dependent Exploration
- `normalize_advantage=False`: N√£o normalizar a vantagem

### 5. Sistema de Checkpoints

```python
checkpoint_callback = CheckpointCallback(
    save_freq=50000,
    save_path="./models/checkpoints/",
    name_prefix="model"
)
```

**Justificativas:**
- `save_freq=50000`: Salva a cada 50k passos para n√£o sobrecarregar o disco
- Estrutura de diret√≥rios separada para cada algoritmo
- Prefixos espec√≠ficos para f√°cil identifica√ß√£o

### 6. Processo de Treinamento

```python
model.learn(
    total_timesteps=1000000,
    callback=checkpoint_callback,
    progress_bar=True
)
```

**Explica√ß√£o:**
- `total_timesteps=1000000`: 1M passos para converg√™ncia adequada
- `progress_bar=True`: Feedback visual do progresso
- Uso de callback para salvar checkpoints

### 7. Salvamento do Modelo

```python
model.save("models/model_final")
```

**Justificativas:**
- Salvamento do modelo final para uso posterior
- Estrutura de diret√≥rios organizada
- Nomenclatura clara e consistente

## üìä M√©tricas de Avalia√ß√£o

### 1. Recompensa M√©dia
- Calculada sobre 10 epis√≥dios
- Indica desempenho geral do modelo
- Valores esperados:
  - DQN: ~-85 pontos
  - A2C: ~-82 pontos
  - PPO: ~-78 pontos

### 2. Estabilidade (Desvio Padr√£o)
- Medida de consist√™ncia do desempenho
- Valores esperados:
  - DQN: ~35 pontos
  - A2C: ~28 pontos
  - PPO: ~22 pontos

### 3. Tempo de Avalia√ß√£o
- Medido em segundos por epis√≥dio
- Importante para aplica√ß√µes em tempo real
- Valores t√≠picos:
  - DQN: ~0.16s
  - A2C: ~0.13s
  - PPO: ~0.15s

## üîÑ Fluxo de Trabalho Recomendado

1. **Treinamento Inicial**
   - Execute cada algoritmo separadamente
   - Monitore o progresso no TensorBoard
   - Verifique a converg√™ncia

2. **Avalia√ß√£o**
   - Use o script comparador.py
   - Analise os gr√°ficos gerados
   - Compare com os resultados esperados

3. **Otimiza√ß√£o**
   - Ajuste hiperpar√¢metros se necess√°rio
   - Foque em melhorar a estabilidade
   - Mantenha o tempo de avalia√ß√£o baixo

4. **Documenta√ß√£o**
   - Registre os resultados
   - Atualize os gr√°ficos
   - Mantenha o README atualizado

## üìö Refer√™ncias

1. [Stable-Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
2. [Gymnasium Documentation](https://gymnasium.farama.org/)
3. [Acrobot-v1 Environment](https://gymnasium.farama.org/environments/classic_control/acrobot/)
4. [DQN Paper](https://www.nature.com/articles/nature14236)
5. [A2C Paper](https://arxiv.org/abs/1602.01783)
6. [PPO Paper](https://arxiv.org/abs/1707.06347)

## ü§ù Contribuindo

Contribui√ß√µes s√£o bem-vindas! Por favor, sinta-se √† vontade para:
1. Reportar bugs
2. Sugerir melhorias
3. Adicionar novos algoritmos
4. Melhorar a documenta√ß√£o

## üìù Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes. 

## üéØ Objetivo
O objetivo deste ambiente √© balan√ßar um p√™ndulo duplo at√© uma altura alvo, aplicando torque na junta entre os dois elos.

## üìà An√°lise

### Melhor Algoritmo: PPO
- **Recompensa M√©dia**: -78.5
- **Estabilidade**: Alta (desvio padr√£o 22.1)
- **Converg√™ncia**: R√°pida (200k passos)

## üöÄ Como Usar

### 1. Treinamento
```bash
python train_dqn_acro.py
python train_a2c_acro.py
python train_ppo_acro.py
```

### 2. Compara√ß√£o
```bash
python comparador.py
```

## üìä Visualiza√ß√£o

### Gr√°ficos
- [Recompensa M√©dia](resultados/recompensa_media.png)
- [Estabilidade](resultados/estabilidade_recompensa.png)
- [Tempo de Avalia√ß√£o](resultados/tempo_avaliacao.png)

### Tabelas
- [Tabela Comparativa](resultados/tabela_comparativa.csv)
- [Tabela de Converg√™ncia](resultados/tabela_convergencia.csv)

## üîô [Voltar ao README Principal](../README.md) 