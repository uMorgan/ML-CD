# ğŸ§  ReforÃ§o Inteligente: Estudo e ComparaÃ§Ã£o de Algoritmos de Aprendizado por ReforÃ§o

Este repositÃ³rio foi desenvolvido como parte de um projeto de **Trabalho de ConclusÃ£o de Curso (TCC)**, com o objetivo de estudar, treinar e comparar algoritmos de **Aprendizado por ReforÃ§o (Reinforcement Learning)** em ambientes clÃ¡ssicos do **Gymnasium** (anteriormente OpenAI Gym). Atualmente, os ambientes abordados sÃ£o `CartPole-v1`, `Acrobot-v1` e `LunarLander-v2`.

## ğŸ¯ Objetivo

O propÃ³sito deste projeto Ã© realizar uma anÃ¡lise prÃ¡tica e comparativa entre os seguintes algoritmos:

- `DQN` (Deep Q-Network)
- `A2C` (Advantage Actor-Critic)
- `PPO` (Proximal Policy Optimization)

A comparaÃ§Ã£o Ã© feita por meio de treinamento por geraÃ§Ãµes, permitindo salvar e analisar o desempenho de cada agente ao longo do tempo em diferentes ambientes.

## ğŸ“š Finalidade AcadÃªmica

Este repositÃ³rio tem **finalidade exclusivamente acadÃªmica**, voltado ao estudo de tÃ©cnicas modernas de aprendizado por reforÃ§o e seu desempenho em ambientes clÃ¡ssicos. Os experimentos realizados aqui embasam parte do conteÃºdo e das conclusÃµes do TCC.

## ğŸ› ï¸ ConteÃºdo

O repositÃ³rio inclui:

- Scripts de treinamento para cada algoritmo nos diferentes ambientes.
- Checkpoints salvos por geraÃ§Ã£o durante o treinamento.
- Modelos finais treinados.
- Scripts para avaliaÃ§Ã£o comparativa com geraÃ§Ã£o de tabelas e grÃ¡ficos de desempenho.
- MÃ©tricas avaliadas: Recompensa mÃ©dia, estabilidade, tempo de convergÃªncia e tempo de execuÃ§Ã£o.

---

## Como Usar

Para utilizar este repositÃ³rio, siga os passos abaixo:

### 1. InstalaÃ§Ã£o

Certifique-se de ter o Python instalado em seu sistema. Recomenda-se o uso de um ambiente virtual.

Clone o repositÃ³rio:

```bash
git clone https://github.com/uMorgan/ML-CD.git
```

Instale as dependÃªncias listadas no arquivo `requirements.txt`:

```bash
pip install -r requirements.txt
```

### 2. Treinamento dos Modelos

Para treinar os modelos para um ambiente especÃ­fico, navegue atÃ© o diretÃ³rio correspondente (`CartPole/`, `Acrobot/` ou `LunarLander/`) e execute os scripts de treinamento:

```bash
# Exemplo para CartPole com DQN
cd CartPole/
python dqn_cart_train.py

# Navegue e execute os scripts para outros algoritmos e ambientes conforme necessÃ¡rio.
# Ex: cd ../Acrobot/ && python train_a2c_acro.py
```

Os modelos treinados serÃ£o salvos na pasta `models/` dentro de cada diretÃ³rio de ambiente, e os logs de treinamento (para visualizaÃ§Ã£o com TensorBoard) na pasta `logs/`.

### 3. ComparaÃ§Ã£o de Algoritmos por Ambiente

ApÃ³s treinar os modelos, vocÃª pode comparar o desempenho dos algoritmos em um ambiente. Navegue atÃ© o diretÃ³rio do ambiente e execute o script `comparador.py`:

```bash
# Exemplo para LunarLander
cd LunarLander/
python comparador.py
```

Este script gerarÃ¡ uma tabela comparativa (`tabela_comparativa*.csv`) e grÃ¡ficos na pasta `resultados/` do ambiente. VocÃª pode encontrar mais detalhes sobre os resultados e os grÃ¡ficos especÃ­ficos no README de cada ambiente ([CartPole/README.md](CartPole/README.md), [Acrobot/README.md](Acrobot/README.md), [LunarLander/README.md](LunarLander/README.md)).

### 4. Resultados Finais: ComparaÃ§Ã£o entre Ambientes

Para uma visÃ£o geral do desempenho final dos algoritmos em todos os ambientes, execute o script `resultado_final.py` na raiz do projeto:

```bash
python resultado_final.py
```

Este script lÃª os resultados das tabelas comparativas de cada ambiente (geradas no passo 3) e produz um grÃ¡fico que compara a recompensa mÃ©dia final de cada algoritmo em CartPole, Acrobot e LunarLander. O grÃ¡fico Ã© salvo em `resultados/comparacao_algoritmos_ambientes.png` na raiz do projeto.

## Resultados Gerais

Aqui estÃ¡ uma comparaÃ§Ã£o visual do desempenho final dos algoritmos nos diferentes ambientes:

![ComparaÃ§Ã£o de Algoritmos por Ambiente](resultados/comparacao_algoritmos_ambientes.png)

*(Para anÃ¡lises mais detalhadas e grÃ¡ficos especÃ­ficos por ambiente, consulte os READMEs em [CartPole/](CartPole/), [Acrobot/](Acrobot/) e [LunarLander/](LunarLander/).)*

## Estrutura do Projeto

A estrutura principal do projeto Ã© a seguinte:

```
ML-CD-main/
â”œâ”€â”€ Acrobot/              # Scripts e resultados para o ambiente Acrobot
â”‚   â”œâ”€â”€ models/         # Modelos treinados
â”‚   â”œâ”€â”€ logs/           # Logs do TensorBoard
â”‚   â””â”€â”€ resultados/     # Tabelas e grÃ¡ficos de comparaÃ§Ã£o do Acrobot
â”œâ”€â”€ CartPole/             # Scripts e resultados para o ambiente CartPole
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ resultados/     # Tabelas e grÃ¡ficos de comparaÃ§Ã£o do CartPole
â”œâ”€â”€ LunarLander/          # Scripts e resultados para o ambiente LunarLander
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ resultados/     # Tabelas e grÃ¡ficos de comparaÃ§Ã£o do LunarLander
â”œâ”€â”€ resultados/           # Resultados gerais (grÃ¡fico comparativo entre ambientes)
â”‚   â””â”€â”€ comparacao_algoritmos_ambientes.png
â”œâ”€â”€ .gitignore            # Arquivos e pastas a serem ignorados pelo Git
â”œâ”€â”€ README.md             # Este arquivo
â”œâ”€â”€ requirements.txt      # DependÃªncias do projeto
â””â”€â”€ resultado_final.py    # Script para gerar o grÃ¡fico comparativo geral
```

*(Os scripts de treinamento, comparaÃ§Ã£o e visualizaÃ§Ã£o especÃ­ficos de cada ambiente estÃ£o localizados dentro de seus respectivos diretÃ³rios.)*

## ğŸ“š Finalidade AcadÃªmica

Este repositÃ³rio tem **finalidade exclusivamente acadÃªmica**, voltado ao estudo de tÃ©cnicas modernas de aprendizado por reforÃ§o e seu desempenho em ambientes clÃ¡ssicos. Os experimentos realizados aqui embasam parte do conteÃºdo e das conclusÃµes do TCC.

## Contato

Se tiver alguma dÃºvida ou sugestÃ£o, sinta-se Ã  vontade para entrar em contato:

- **Autor:** [JoÃ£o Morgan De Almeida Lins Do Vale] (morgantaria965@gmail.com)

---

Este projeto representa um esforÃ§o de aprendizado e aplicaÃ§Ã£o prÃ¡tica de conceitos fundamentais e avanÃ§ados em inteligÃªncia artificial, com foco em aprendizado por reforÃ§o.
