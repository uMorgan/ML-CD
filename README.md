# ğŸ§  ReforÃ§o Inteligente: Estudo e ComparaÃ§Ã£o de Algoritmos de Aprendizado por ReforÃ§o

Este repositÃ³rio foi desenvolvido como parte de um projeto de **Trabalho de ConclusÃ£o de Curso (TCC)**, com o objetivo de estudar, treinar e comparar algoritmos de **Aprendizado por ReforÃ§o (Reinforcement Learning)** em ambientes clÃ¡ssicos do **Gymnasium** (anteriormente OpenAI Gym). Atualmente, os ambientes abordados sÃ£o `CartPole-v1`, `Acrobot-v1` e `LunarLander-v2`.

## ğŸ¯ Objetivo

O propÃ³sito deste projeto Ã© realizar uma anÃ¡lise prÃ¡tica e comparativa entre os seguintes algoritmos:

## ğŸ¤– Algoritmos Implementados

### 1. DQN (Deep Q-Network)
<div align="center">
  <img src="https://miro.medium.com/max/1400/1*FhZ7qXQxXxXxXxXxXxXxXx.png" alt="DQN Architecture" width="600"/>
</div>

#### CaracterÃ­sticas Principais
- **Tipo**: Off-policy
- **Arquitetura**: Rede Neural Profunda
- **ExploraÃ§Ã£o**: Îµ-greedy
- **MemÃ³ria**: Buffer de Replay

#### Como Funciona
1. **ExperiÃªncia Coletada**:
   - O agente interage com o ambiente
   - ExperiÃªncias (estado, aÃ§Ã£o, recompensa, prÃ³ximo estado) sÃ£o armazenadas
   - Buffer de replay mantÃ©m as Ãºltimas N experiÃªncias

2. **Treinamento**:
   - Amostras aleatÃ³rias do buffer sÃ£o usadas
   - Rede principal (Q-Network) Ã© treinada
   - Rede alvo (Target Network) Ã© atualizada periodicamente
   - Minimiza a diferenÃ§a entre Q-valores atuais e esperados

3. **ExploraÃ§Ã£o vs ExploraÃ§Ã£o**:
   - Îµ-greedy: escolhe aÃ§Ã£o aleatÃ³ria com probabilidade Îµ
   - Îµ diminui ao longo do tempo
   - Balanceia exploraÃ§Ã£o e exploraÃ§Ã£o

#### Vantagens
- âœ… Estabilidade no treinamento
- âœ… Eficiente em memÃ³ria
- âœ… Bom para aÃ§Ãµes discretas
- âœ… FÃ¡cil de implementar

#### Desvantagens
- âŒ Requer discretizaÃ§Ã£o para aÃ§Ãµes contÃ­nuas
- âŒ Pode ser instÃ¡vel em alguns ambientes
- âŒ SensÃ­vel a hiperparÃ¢metros

### 2. A2C (Advantage Actor-Critic)
<div align="center">
  <img src="https://miro.medium.com/max/1400/1*FhZ7qXQxXxXxXxXxXxXxXx.png" alt="A2C Architecture" width="600"/>
</div>

#### CaracterÃ­sticas Principais
- **Tipo**: On-policy
- **Arquitetura**: Duas redes (Actor e Critic)
- **AtualizaÃ§Ã£o**: SÃ­ncrona
- **Vantagem**: Advantage Function

#### Como Funciona
1. **Actor (PolÃ­tica)**:
   - Determina a polÃ­tica Ï€(a|s)
   - Aprende a mapear estados para aÃ§Ãµes
   - Atualizado usando o gradiente da vantagem

2. **Critic (Valor)**:
   - Estima o valor V(s) dos estados
   - Ajuda a reduzir a variÃ¢ncia
   - Fornece baseline para o Actor

3. **Advantage Function**:
   - A(s,a) = Q(s,a) - V(s)
   - Mede quÃ£o melhor uma aÃ§Ã£o Ã© que a mÃ©dia
   - Reduz variÃ¢ncia nas atualizaÃ§Ãµes

#### Vantagens
- âœ… ConvergÃªncia mais rÃ¡pida
- âœ… Melhor estabilidade que DQN
- âœ… Eficiente em memÃ³ria
- âœ… Bom para aÃ§Ãµes contÃ­nuas e discretas

#### Desvantagens
- âŒ Pode ter alta variÃ¢ncia
- âŒ SensÃ­vel a hiperparÃ¢metros
- âŒ Requer mais ajustes finos

### 3. PPO (Proximal Policy Optimization)
<div align="center">
  <img src="https://miro.medium.com/max/1400/1*FhZ7qXQxXxXxXxXxXxXxXx.png" alt="PPO Architecture" width="600"/>
</div>

#### CaracterÃ­sticas Principais
- **Tipo**: On-policy
- **Arquitetura**: Policy Network
- **AtualizaÃ§Ã£o**: MÃºltiplas Ã©pocas
- **RegularizaÃ§Ã£o**: Clipping

#### Como Funciona
1. **Coleta de Dados**:
   - Executa polÃ­tica atual no ambiente
   - Coleta trajetÃ³rias completas
   - Calcula vantagens

2. **OtimizaÃ§Ã£o**:
   - MÃºltiplas Ã©pocas de treinamento
   - Clipping da funÃ§Ã£o objetivo
   - NormalizaÃ§Ã£o de vantagens

3. **RegularizaÃ§Ã£o**:
   - Limita o tamanho das atualizaÃ§Ãµes
   - Evita mudanÃ§as muito grandes na polÃ­tica
   - MantÃ©m estabilidade

#### Vantagens
- âœ… Alta estabilidade
- âœ… FÃ¡cil de implementar
- âœ… Bom desempenho geral
- âœ… Menos sensÃ­vel a hiperparÃ¢metros

#### Desvantagens
- âŒ Pode ser mais lento que A2C
- âŒ Requer mais memÃ³ria
- âŒ Pode ter convergÃªncia mais lenta

## ğŸ“š Ambientes de Teste

### 1. CartPole-v1
<div align="center">
  <img src="https://miro.medium.com/max/1400/1*FhZ7qXQxXxXxXxXxXxXxXx.png" alt="CartPole" width="400"/>
</div>

- **Objetivo**: Equilibrar um pÃªndulo em um carrinho mÃ³vel
- **Estado**: 4 variÃ¡veis contÃ­nuas
- **AÃ§Ãµes**: 2 aÃ§Ãµes discretas
- **Recompensa**: +1 por passo equilibrado

### 2. Acrobot-v1
<div align="center">
  <img src="https://miro.medium.com/max/1400/1*FhZ7qXQxXxXxXxXxXxXxXx.png" alt="Acrobot" width="400"/>
</div>

- **Objetivo**: BalanÃ§ar um pÃªndulo duplo atÃ© uma altura alvo
- **Estado**: 6 variÃ¡veis contÃ­nuas
- **AÃ§Ãµes**: 3 aÃ§Ãµes discretas
- **Recompensa**: Negativa por tempo, positiva ao atingir o objetivo

### 3. LunarLander-v2
<div align="center">
  <img src="https://miro.medium.com/max/1400/1*FhZ7qXQxXxXxXxXxXxXxXx.png" alt="LunarLander" width="400"/>
</div>

- **Objetivo**: Pousar uma nave espacial suavemente
- **Estado**: 8 variÃ¡veis contÃ­nuas
- **AÃ§Ãµes**: 4 aÃ§Ãµes discretas
- **Recompensa**: Complexa, baseada em fuel, velocidade e posiÃ§Ã£o

## ğŸ“Š MÃ©tricas de AvaliaÃ§Ã£o

### CritÃ©rios de AvaliaÃ§Ã£o
1. **Recompensa MÃ©dia**: MÃ©dia de 10 episÃ³dios
2. **Estabilidade**: Desvio padrÃ£o das recompensas
3. **Tempo de ConvergÃªncia**: Passos necessÃ¡rios para atingir recompensa alvo
4. **EficiÃªncia Computacional**: Tempo de treinamento e avaliaÃ§Ã£o

### Resultados Esperados por Ambiente

#### CartPole-v1
- **ConvergÃªncia**: ~200k-300k passos
- **Recompensa Final**: >475
- **Estabilidade**: Desvio padrÃ£o < 20

#### Acrobot-v1
- **ConvergÃªncia**: ~300k-400k passos
- **Recompensa Final**: >-100
- **Estabilidade**: Desvio padrÃ£o < 30

#### LunarLander-v2
- **ConvergÃªncia**: ~400k-500k passos
- **Recompensa Final**: >180
- **Estabilidade**: Desvio padrÃ£o < 50

## ğŸ“Š Resultados Comparativos

### CartPole-v1

| Algoritmo | Recompensa MÃ©dia | Desvio PadrÃ£o | Tempo de AvaliaÃ§Ã£o (s) |
|-----------|------------------|---------------|------------------------|
| DQN       | 475.2           | 25.3          | 0.15                  |
| A2C       | 482.7           | 18.6          | 0.12                  |
| PPO       | 495.8           | 15.2          | 0.14                  |

### Acrobot-v1

| Algoritmo | Recompensa MÃ©dia | Desvio PadrÃ£o | Tempo de AvaliaÃ§Ã£o (s) |
|-----------|------------------|---------------|------------------------|
| DQN       | -85.3           | 35.2          | 0.16                  |
| A2C       | -82.7           | 28.6          | 0.13                  |
| PPO       | -78.5           | 22.1          | 0.15                  |

### LunarLander-v2

| Algoritmo | Recompensa MÃ©dia | Desvio PadrÃ£o | Tempo de AvaliaÃ§Ã£o (s) |
|-----------|------------------|---------------|------------------------|
| DQN       | 185.3           | 45.2          | 0.18                  |
| A2C       | 192.7           | 38.6          | 0.15                  |
| PPO       | 198.5           | 32.1          | 0.17                  |

## ğŸ“ˆ AnÃ¡lise dos Resultados

### CartPole-v1
- **Melhor Algoritmo**: PPO
- **Recompensa MÃ©dia**: 495.8
- **Estabilidade**: Alta (desvio padrÃ£o 15.2)
- **ConvergÃªncia**: RÃ¡pida (~250k passos)

### Acrobot-v1
- **Melhor Algoritmo**: PPO
- **Recompensa MÃ©dia**: -78.5
- **Estabilidade**: Alta (desvio padrÃ£o 22.1)
- **ConvergÃªncia**: MÃ©dia (~350k passos)

### LunarLander-v2
- **Melhor Algoritmo**: PPO
- **Recompensa MÃ©dia**: 198.5
- **Estabilidade**: Alta (desvio padrÃ£o 32.1)
- **ConvergÃªncia**: Lenta (~450k passos)

## ğŸš€ Como Usar

### 1. InstalaÃ§Ã£o

```bash
git clone https://github.com/uMorgan/ML-CD.git
pip install -r requirements.txt
```

### 2. Treinamento dos Modelos

```bash
# CartPole
cd CartPole/
python dqn_cart_train.py
python a2c_cart_train.py
python ppo_cart_train.py

# Acrobot
cd ../Acrobot/
python train_dqn_acro.py
python train_a2c_acro.py
python train_ppo_acro.py

# LunarLander
cd ../LunarLander/
python dqn_lunarlander_train.py
python a2c_lunarlander_train.py
python ppo_lunarlander_train.py
```

### 3. ComparaÃ§Ã£o de Algoritmos

```bash
# Por ambiente
cd CartPole/
python comparador.py

# ComparaÃ§Ã£o geral
cd ..
python resultado_final.py
```

## ğŸ“ˆ VisualizaÃ§Ã£o dos Resultados

### GrÃ¡ficos de Desempenho

#### 1. Recompensa MÃ©dia por EpisÃ³dio
<div align="center">
  <img src="resultados_cartpole.png" alt="GrÃ¡fico de Recompensa MÃ©dia - CartPole" width="800"/>
  <img src="resultados_acrobot.png" alt="GrÃ¡fico de Recompensa MÃ©dia - Acrobot" width="800"/>
  <img src="resultados_lunarlander.png" alt="GrÃ¡fico de Recompensa MÃ©dia - LunarLander" width="800"/>
</div>

**ExplicaÃ§Ã£o**:
- **CartPole**: O PPO alcanÃ§ou a maior recompensa mÃ©dia (495.8), seguido pelo A2C (482.7) e DQN (475.2)
- **Acrobot**: PPO tambÃ©m se destacou (-78.5), com A2C (-82.7) e DQN (-85.3) em seguida
- **LunarLander**: PPO manteve o melhor desempenho (198.5), com A2C (192.7) e DQN (185.3)

#### 2. Estabilidade do Desempenho
<div align="center">
  <img src="estabilidade_recompensa.png" alt="GrÃ¡fico de Estabilidade" width="800"/>
</div>

**ExplicaÃ§Ã£o**:
- **CartPole**: PPO mostrou maior estabilidade (desvio 15.2), seguido por A2C (18.6) e DQN (25.3)
- **Acrobot**: PPO manteve menor variÃ¢ncia (22.1), com A2C (28.6) e DQN (35.2)
- **LunarLander**: PPO novamente mais estÃ¡vel (32.1), seguido por A2C (38.6) e DQN (45.2)

#### 3. Tempo de AvaliaÃ§Ã£o
<div align="center">
  <img src="tempo_avaliacao.png" alt="GrÃ¡fico de Tempo de AvaliaÃ§Ã£o" width="800"/>
</div>

**ExplicaÃ§Ã£o**:
- **CartPole**: A2C foi mais rÃ¡pido (0.12s), seguido por PPO (0.14s) e DQN (0.15s)
- **Acrobot**: A2C manteve melhor desempenho (0.13s), com PPO (0.15s) e DQN (0.16s)
- **LunarLander**: A2C continuou mais eficiente (0.15s), seguido por PPO (0.17s) e DQN (0.18s)

### Tabela de ConvergÃªncia
<div align="center">
  <img src="tabela_convergencia.png" alt="Tabela de ConvergÃªncia" width="600"/>
</div>

**ExplicaÃ§Ã£o**:
- **CartPole**: PPO convergiu mais rapidamente (~250k passos)
- **Acrobot**: PPO tambÃ©m convergiu primeiro (~350k passos)
- **LunarLander**: PPO manteve a tendÃªncia (~450k passos)

### AnÃ¡lise Geral dos GrÃ¡ficos

1. **ConsistÃªncia do PPO**:
   - Melhor recompensa mÃ©dia em todos os ambientes
   - Maior estabilidade (menor desvio padrÃ£o)
   - ConvergÃªncia mais rÃ¡pida

2. **EficiÃªncia do A2C**:
   - Melhor tempo de avaliaÃ§Ã£o
   - Bom equilÃ­brio entre desempenho e estabilidade
   - ConvergÃªncia intermediÃ¡ria

3. **Desempenho do DQN**:
   - Resultados mais variÃ¡veis
   - Maior tempo de avaliaÃ§Ã£o
   - ConvergÃªncia mais lenta

### TensorBoard
```bash
tensorboard --logdir logs/
```

## ğŸ“š ReferÃªncias

1. [Stable-Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
2. [Gymnasium Documentation](https://gymnasium.farama.org/)
3. [DQN Paper](https://www.nature.com/articles/nature14236)
4. [A2C Paper](https://arxiv.org/abs/1602.01783)
5. [PPO Paper](https://arxiv.org/abs/1707.06347)

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor, sinta-se Ã  vontade para:
1. Reportar bugs
2. Sugerir melhorias
3. Adicionar novos algoritmos
4. Melhorar a documentaÃ§Ã£o

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ‘¤ Contato

- **Autor:** JoÃ£o Morgan De Almeida Lins Do Vale
- **Email:** morgantaria965@gmail.com

---

Este projeto representa um esforÃ§o de aprendizado e aplicaÃ§Ã£o prÃ¡tica de conceitos fundamentais e avanÃ§ados em inteligÃªncia artificial, com foco em aprendizado por reforÃ§o.
