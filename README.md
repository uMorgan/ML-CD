# üß† Refor√ßo Inteligente: Estudo e Compara√ß√£o de Algoritmos de Aprendizado por Refor√ßo

Este reposit√≥rio foi desenvolvido como parte de um projeto de **Trabalho de Conclus√£o de Curso (TCC)**, com o objetivo de estudar, treinar e comparar algoritmos de **Aprendizado por Refor√ßo (Reinforcement Learning)** em ambientes cl√°ssicos do **Gymnasium** (anteriormente OpenAI Gym). Atualmente, os ambientes abordados s√£o `CartPole-v1`, `Acrobot-v1` e `LunarLander-v2`.

## üéØ Objetivo

O prop√≥sito deste projeto √© realizar uma an√°lise pr√°tica e comparativa entre os seguintes algoritmos:

## ü§ñ Algoritmos Implementados

### 1. DQN (Deep Q-Network)
<div align="center">
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/DQN.png" alt="DQN Architecture" width="600"/>
</div>

#### Caracter√≠sticas Principais
- **Tipo**: Off-policy
- **Arquitetura**: Rede Neural Profunda
- **Explora√ß√£o**: Œµ-greedy
- **Mem√≥ria**: Buffer de Replay

#### Como Funciona
1. **Experi√™ncia Coletada**:
   - O agente interage com o ambiente
   - Experi√™ncias (estado, a√ß√£o, recompensa, pr√≥ximo estado) s√£o armazenadas
   - Buffer de replay mant√©m as √∫ltimas N experi√™ncias

2. **Treinamento**:
   - Amostras aleat√≥rias do buffer s√£o usadas
   - Rede principal (Q-Network) √© treinada
   - Rede alvo (Target Network) √© atualizada periodicamente
   - Minimiza a diferen√ßa entre Q-valores atuais e esperados

3. **Explora√ß√£o vs Explora√ß√£o**:
   - Œµ-greedy: escolhe a√ß√£o aleat√≥ria com probabilidade Œµ
   - Œµ diminui ao longo do tempo
   - Balanceia explora√ß√£o e explora√ß√£o

#### Vantagens
- ‚úÖ Estabilidade no treinamento
- ‚úÖ Eficiente em mem√≥ria
- ‚úÖ Bom para a√ß√µes discretas
- ‚úÖ F√°cil de implementar

#### Desvantagens
- ‚ùå Requer discretiza√ß√£o para a√ß√µes cont√≠nuas
- ‚ùå Pode ser inst√°vel em alguns ambientes
- ‚ùå Sens√≠vel a hiperpar√¢metros

### 2. A2C (Advantage Actor-Critic)
<div align="center">
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/A2C.png" alt="A2C Architecture" width="600"/>
</div>

#### Caracter√≠sticas Principais
- **Tipo**: On-policy
- **Arquitetura**: Duas redes (Actor e Critic)
- **Atualiza√ß√£o**: S√≠ncrona
- **Vantagem**: Advantage Function

#### Como Funciona
1. **Actor (Pol√≠tica)**:
   - Determina a pol√≠tica œÄ(a|s)
   - Aprende a mapear estados para a√ß√µes
   - Atualizado usando o gradiente da vantagem

2. **Critic (Valor)**:
   - Estima o valor V(s) dos estados
   - Ajuda a reduzir a vari√¢ncia
   - Fornece baseline para o Actor

3. **Advantage Function**:
   - A(s,a) = Q(s,a) - V(s)
   - Mede qu√£o melhor uma a√ß√£o √© que a m√©dia
   - Reduz vari√¢ncia nas atualiza√ß√µes

#### Vantagens
- ‚úÖ Converg√™ncia mais r√°pida
- ‚úÖ Melhor estabilidade que DQN
- ‚úÖ Eficiente em mem√≥ria
- ‚úÖ Bom para a√ß√µes cont√≠nuas e discretas

#### Desvantagens
- ‚ùå Pode ter alta vari√¢ncia
- ‚ùå Sens√≠vel a hiperpar√¢metros
- ‚ùå Requer mais ajustes finos

### 3. PPO (Proximal Policy Optimization)
<div align="center">
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/PPO.png" alt="PPO Architecture" width="600"/>
</div>

#### Caracter√≠sticas Principais
- **Tipo**: On-policy
- **Arquitetura**: Policy Network
- **Atualiza√ß√£o**: M√∫ltiplas √©pocas
- **Regulariza√ß√£o**: Clipping

#### Como Funciona
1. **Coleta de Dados**:
   - Executa pol√≠tica atual no ambiente
   - Coleta trajet√≥rias completas
   - Calcula vantagens

2. **Otimiza√ß√£o**:
   - M√∫ltiplas √©pocas de treinamento
   - Clipping da fun√ß√£o objetivo
   - Normaliza√ß√£o de vantagens

3. **Regulariza√ß√£o**:
   - Limita o tamanho das atualiza√ß√µes
   - Evita mudan√ßas muito grandes na pol√≠tica
   - Mant√©m estabilidade

#### Vantagens
- ‚úÖ Alta estabilidade
- ‚úÖ F√°cil de implementar
- ‚úÖ Bom desempenho geral
- ‚úÖ Menos sens√≠vel a hiperpar√¢metros

#### Desvantagens
- ‚ùå Pode ser mais lento que A2C
- ‚ùå Requer mais mem√≥ria
- ‚ùå Pode ter converg√™ncia mais lenta

## üìö Ambientes de Teste

### 1. CartPole-v1
<div align="center">
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/cartpole.png" alt="CartPole" width="400"/>
</div>

- **Objetivo**: Equilibrar um p√™ndulo em um carrinho m√≥vel
- **Estado**: 4 vari√°veis cont√≠nuas
- **A√ß√µes**: 2 a√ß√µes discretas
- **Recompensa**: +1 por passo equilibrado

### 2. Acrobot-v1
<div align="center">
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/acrobot.png" alt="Acrobot" width="400"/>
</div>

- **Objetivo**: Balan√ßar um p√™ndulo duplo at√© uma altura alvo
- **Estado**: 6 vari√°veis cont√≠nuas
- **A√ß√µes**: 3 a√ß√µes discretas
- **Recompensa**: Negativa por tempo, positiva ao atingir o objetivo

### 3. LunarLander-v2
<div align="center">
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/lunarlander.png" alt="LunarLander" width="400"/>
</div>

- **Objetivo**: Pousar uma nave espacial suavemente
- **Estado**: 8 vari√°veis cont√≠nuas
- **A√ß√µes**: 4 a√ß√µes discretas
- **Recompensa**: Complexa, baseada em fuel, velocidade e posi√ß√£o

## üìä M√©tricas de Avalia√ß√£o

### Crit√©rios de Avalia√ß√£o
1. **Recompensa M√©dia**: M√©dia de 10 epis√≥dios
2. **Estabilidade**: Desvio padr√£o das recompensas
3. **Tempo de Converg√™ncia**: Passos necess√°rios para atingir recompensa alvo
4. **Efici√™ncia Computacional**: Tempo de treinamento e avalia√ß√£o

### Resultados Esperados por Ambiente

#### CartPole-v1
- **Converg√™ncia**: ~200k-300k passos
- **Recompensa Final**: >475
- **Estabilidade**: Desvio padr√£o < 20

#### Acrobot-v1
- **Converg√™ncia**: ~300k-400k passos
- **Recompensa Final**: >-100
- **Estabilidade**: Desvio padr√£o < 30

#### LunarLander-v2
- **Converg√™ncia**: ~400k-500k passos
- **Recompensa Final**: >180
- **Estabilidade**: Desvio padr√£o < 50

## üìä Resultados Comparativos

### CartPole-v1

| Algoritmo | Recompensa M√©dia | Desvio Padr√£o | Tempo de Avalia√ß√£o (s) |
|-----------|------------------|---------------|------------------------|
| DQN       | 475.2           | 25.3          | 0.15                  |
| A2C       | 482.7           | 18.6          | 0.12                  |
| PPO       | 495.8           | 15.2          | 0.14                  |

<div align="center">
  <img src="resultados/resultados_cartpole.png" alt="Gr√°fico de Recompensa M√©dia - CartPole" width="800"/>
</div>

### Acrobot-v1

| Algoritmo | Recompensa M√©dia | Desvio Padr√£o | Tempo de Avalia√ß√£o (s) |
|-----------|------------------|---------------|------------------------|
| DQN       | -85.3           | 35.2          | 0.16                  |
| A2C       | -82.7           | 28.6          | 0.13                  |
| PPO       | -78.5           | 22.1          | 0.15                  |

<div align="center">
  <img src="resultados/resultados_acrobot.png" alt="Gr√°fico de Recompensa M√©dia - Acrobot" width="800"/>
</div>

### LunarLander-v2

| Algoritmo | Recompensa M√©dia | Desvio Padr√£o | Tempo de Avalia√ß√£o (s) |
|-----------|------------------|---------------|------------------------|
| DQN       | 185.3           | 45.2          | 0.18                  |
| A2C       | 192.7           | 38.6          | 0.15                  |
| PPO       | 198.5           | 32.1          | 0.17                  |

<div align="center">
  <img src="resultados/resultados_lunarlander.png" alt="Gr√°fico de Recompensa M√©dia - LunarLander" width="800"/>
</div>

## üìà An√°lise dos Resultados

### Tabela de Converg√™ncia
<div align="center">

| Ambiente | Algoritmo | Epis√≥dios at√© Converg√™ncia |
|----------|-----------|---------------------------|
| CartPole | DQN       | 700k                      |
| CartPole | A2C       | 100k                      |
| CartPole | PPO       | 100k                      |
| Acrobot  | DQN       | 500k                      |
| Acrobot  | A2C       | 300k                      |
| Acrobot  | PPO       | 200k                      |
| LunarLander | DQN    | 700k                      |
| LunarLander | A2C    | 1000k                     |
| LunarLander | PPO    | 100k                      |

</div>

### CartPole-v1
- **Melhor Algoritmo**: PPO e A2C (empate)
- **Recompensa M√©dia**: 495.8 (PPO) e 482.7 (A2C)
- **Estabilidade**: Alta (desvio padr√£o 15.2 para PPO, 18.6 para A2C)
- **Converg√™ncia**: R√°pida (100k passos para ambos)

### Acrobot-v1
- **Melhor Algoritmo**: PPO
- **Recompensa M√©dia**: -78.5
- **Estabilidade**: Alta (desvio padr√£o 22.1)
- **Converg√™ncia**: R√°pida (200k passos)

### LunarLander-v2
- **Melhor Algoritmo**: PPO
- **Recompensa M√©dia**: 198.5
- **Estabilidade**: Alta (desvio padr√£o 32.1)
- **Converg√™ncia**: R√°pida (100k passos)

## üöÄ Como Usar

### 1. Instala√ß√£o

```bash
git clone https://github.com/uMorgan/ML-CD.git
pip install -r requirements.txt
```

### 2. Treinamento dos Modelos

```bash
# CartPole
cd CartPole/
python dqn_cart_train.py
python a2c_cart_train.py
python ppo_cart_train.py

# Acrobot
cd ../Acrobot/
python train_dqn_acro.py
python train_a2c_acro.py
python train_ppo_acro.py

# LunarLander
cd ../LunarLander/
python dqn_lunarlander_train.py
python a2c_lunarlander_train.py
python ppo_lunarlander_train.py
```

### 3. Compara√ß√£o de Algoritmos

```bash
# Por ambiente
cd CartPole/
python comparador.py

# Compara√ß√£o geral
cd ..
python resultado_final.py
```

## üìà Visualiza√ß√£o dos Resultados

### Gr√°ficos de Desempenho

#### 1. Recompensa M√©dia por Epis√≥dio

**Explica√ß√£o**:
- **CartPole**: O PPO alcan√ßou a maior recompensa m√©dia (495.8), seguido pelo A2C (482.7) e DQN (475.2)
- **Acrobot**: PPO tamb√©m se destacou (-78.5), com A2C (-82.7) e DQN (-85.3) em seguida
- **LunarLander**: PPO manteve o melhor desempenho (198.5), com A2C (192.7) e DQN (185.3)

#### 2. Estabilidade do Desempenho

**Explica√ß√£o**:
- **CartPole**: PPO mostrou maior estabilidade (desvio 15.2), seguido por A2C (18.6) e DQN (25.3)
- **Acrobot**: PPO manteve menor vari√¢ncia (22.1), com A2C (28.6) e DQN (35.2)
- **LunarLander**: PPO novamente mais est√°vel (32.1), seguido por A2C (38.6) e DQN (45.2)

#### 3. Tempo de Avalia√ß√£o

**Explica√ß√£o**:
- **CartPole**: A2C foi mais r√°pido (0.12s), seguido por PPO (0.14s) e DQN (0.15s)
- **Acrobot**: A2C manteve melhor desempenho (0.13s), com PPO (0.15s) e DQN (0.16s)
- **LunarLander**: A2C continuou mais eficiente (0.15s), seguido por PPO (0.17s) e DQN (0.18s)

### Tabela de Converg√™ncia
<div align="center">
  <img src="resultados/tabela_convergencia.csv" alt="Tabela de Converg√™ncia" width="600"/>
</div>

**Explica√ß√£o**:
- **CartPole**: PPO e A2C convergiram mais rapidamente (100k passos)
- **Acrobot**: PPO convergiu primeiro (200k passos), seguido por A2C (300k) e DQN (500k)
- **LunarLander**: PPO convergiu mais r√°pido (100k passos), seguido por DQN (700k) e A2C n√£o convergiu

### An√°lise Geral dos Gr√°ficos

1. **Consist√™ncia do PPO**:
   - Melhor recompensa m√©dia em todos os ambientes
   - Maior estabilidade (menor desvio padr√£o)
   - Converg√™ncia mais r√°pida

2. **Efici√™ncia do A2C**:
   - Melhor tempo de avalia√ß√£o
   - Bom equil√≠brio entre desempenho e estabilidade
   - Converg√™ncia intermedi√°ria

3. **Desempenho do DQN**:
   - Resultados mais vari√°veis
   - Maior tempo de avalia√ß√£o
   - Converg√™ncia mais lenta

### TensorBoard
```bash
tensorboard --logdir logs/
```

## üìö Refer√™ncias

1. [Stable-Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
2. [Gymnasium Documentation](https://gymnasium.farama.org/)
3. [DQN Paper](https://www.nature.com/articles/nature14236)
4. [A2C Paper](https://arxiv.org/abs/1602.01783)
5. [PPO Paper](https://arxiv.org/abs/1707.06347)

## ü§ù Contribuindo

Contribui√ß√µes s√£o bem-vindas! Por favor, sinta-se √† vontade para:
1. Reportar bugs
2. Sugerir melhorias
3. Adicionar novos algoritmos
4. Melhorar a documenta√ß√£o

## üìù Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes.

## üë§ Contato

- **Autor:** Jo√£o Morgan De Almeida Lins Do Vale
- **Email:** morgantaria965@gmail.com

## üîó PARA MELHOR VIZUALIZAR:

### Ambientes
- [CartPole-v1](CartPole/README.md) - Equil√≠brio de p√™ndulo em carrinho m√≥vel
- [Acrobot-v1](Acrobot/README.md) - Balan√ßo de p√™ndulo duplo
- [LunarLander-v2](LunarLander/README.md) - Pouso suave de nave espacial


---

Este projeto representa um esfor√ßo de aprendizado e aplica√ß√£o pr√°tica de conceitos fundamentais e avan√ßados em intelig√™ncia artificial, com foco em aprendizado por refor√ßo.
