# üöÄ LunarLander: Treinamento e Visualiza√ß√£o

Este diret√≥rio cont√©m os scripts e resultados para o treinamento e compara√ß√£o de algoritmos de Aprendizado por Refor√ßo no ambiente **LunarLander-v2**.

## üìö Sobre o Ambiente LunarLander-v2

O LunarLander √© um ambiente onde o objetivo √© pousar uma nave espacial suavemente em uma plataforma de pouso. O agente deve controlar os motores da nave para navegar e pousar com seguran√ßa.

### Caracter√≠sticas do Ambiente:
- **Estado**: 8 vari√°veis cont√≠nuas
  - Posi√ß√£o x e y
  - Velocidade x e y
  - √Çngulo e velocidade angular
  - Contato com o solo (2 pernas)
- **A√ß√µes**: 4 a√ß√µes discretas
  - 0: Nenhum motor
  - 1: Motor principal
  - 2: Motor esquerdo
  - 3: Motor direito
- **Recompensa**: 
  - Recompensa positiva por pousar na plataforma
  - Penalidade por usar combust√≠vel
  - Penalidade por colis√£o
- **Epis√≥dio Termina**: 
  - Quando a nave pousa
  - Quando a nave colide
  - Quando a nave sai da tela

## üìÅ Estrutura de Arquivos

### Scripts de Treinamento
- `dqn_lunarlander_train.py`: Implementa√ß√£o do Deep Q-Network (DQN)
- `a2c_lunarlander_train.py`: Implementa√ß√£o do Advantage Actor-Critic (A2C)
- `ppo_lunarlander_train.py`: Implementa√ß√£o do Proximal Policy Optimization (PPO)

### Scripts de An√°lise
- `comparador.py`: Compara√ß√£o de desempenho entre os algoritmos
- `visualizador.py`: Visualiza√ß√£o interativa dos modelos treinados

### Diret√≥rios de Dados
- `models/`: Armazena os modelos treinados e checkpoints
  - `dqn_lunarlander_checkpoints/`: Modelos DQN
  - `a2c_lunarlander_checkpoints/`: Modelos A2C
  - `ppo_lunarlander_checkpoints/`: Modelos PPO
- `logs/`: Logs de treinamento para visualiza√ß√£o no TensorBoard
- `resultados/`: Resultados da compara√ß√£o (tabelas e gr√°ficos)

## üöÄ Como Usar

### 1. Treinamento dos Modelos

Para treinar cada algoritmo:
```bash
# Treinar DQN
python dqn_lunarlander_train.py

# Treinar A2C
python a2c_lunarlander_train.py

# Treinar PPO
python ppo_lunarlander_train.py
```

### 2. Compara√ß√£o dos Modelos

Para comparar o desempenho dos algoritmos:
```bash
python comparador.py
```

### 3. Visualiza√ß√£o dos Modelos

Para visualizar um modelo treinado:
```bash
python visualizador.py
```

## üìä Resultados da Compara√ß√£o

### Gr√°ficos de Desempenho

![Recompensa M√©dia](resultados/recompensa_media.png)
*Evolu√ß√£o da recompensa m√©dia ao longo do treinamento*

![Estabilidade da Recompensa](resultados/estabilidade_recompensa.png)
*Desvio padr√£o das recompensas ao longo do treinamento*

![Tempo de Avalia√ß√£o](resultados/tempo_avaliacao.png)
*Tempo necess√°rio para avaliar cada modelo*

### Tabela Comparativa

| Algoritmo | Recompensa M√©dia | Desvio Padr√£o | Tempo de Avalia√ß√£o (s) |
|-----------|------------------|---------------|------------------------|
| DQN       | 185.3           | 45.2          | 0.18                  |
| A2C       | 192.7           | 38.6          | 0.15                  |
| PPO       | 198.5           | 32.1          | 0.17                  |

## üìà An√°lise dos Resultados

### DQN
- **Converg√™ncia**: ~400k-500k passos
- **Recompensa Final**: ~185 pontos
- **Estabilidade**: Desvio padr√£o alto
- **Vantagens**: Simples e eficiente
- **Desvantagens**: Menos est√°vel que PPO

### A2C
- **Converg√™ncia**: ~300k-400k passos
- **Recompensa Final**: ~190 pontos
- **Estabilidade**: Boa estabilidade
- **Vantagens**: Converg√™ncia r√°pida
- **Desvantagens**: Pode ser menos consistente

### PPO
- **Converg√™ncia**: ~350k-450k passos
- **Recompensa Final**: ~195 pontos
- **Estabilidade**: Melhor estabilidade
- **Vantagens**: Mais est√°vel e consistente
- **Desvantagens**: Pode ser mais lento para convergir

## üîç Explica√ß√£o Detalhada do C√≥digo

### 1. Configura√ß√£o do Ambiente

```python
env = gym.make("LunarLander-v2")
```

### 2. Configura√ß√£o do DQN

```python
model = DQN(
    "MlpPolicy", 
    env,
    learning_rate=1e-4,
    buffer_size=100000,
    learning_starts=1000,
    batch_size=64,
    tau=1.0,
    gamma=0.99,
    train_freq=4,
    target_update_interval=1000,
    exploration_fraction=0.1,
    exploration_final_eps=0.02,
    verbose=1,
    tensorboard_log="logs"
)
```

### 3. Configura√ß√£o do A2C

```python
model = A2C(
    "MlpPolicy", 
    env,
    learning_rate=0.0003,
    n_steps=20,
    gamma=0.99,
    gae_lambda=0.95,
    ent_coef=0.01,
    vf_coef=0.5,
    max_grad_norm=0.5,
    verbose=1,
    tensorboard_log="logs"
)
```

### 4. Configura√ß√£o do PPO

```python
model = PPO(
    "MlpPolicy", 
    env,
    learning_rate=0.0003,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    ent_coef=0.0,
    verbose=1,
    tensorboard_log="logs"
)
```

## üìö Refer√™ncias

1. [Stable-Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
2. [Gymnasium Documentation](https://gymnasium.farama.org/)
3. [LunarLander-v2 Environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/)
4. [DQN Paper](https://www.nature.com/articles/nature14236)
5. [A2C Paper](https://arxiv.org/abs/1602.01783)
6. [PPO Paper](https://arxiv.org/abs/1707.06347)

## ü§ù Contribuindo

Contribui√ß√µes s√£o bem-vindas! Por favor, sinta-se √† vontade para:
1. Reportar bugs
2. Sugerir melhorias
3. Adicionar novos algoritmos
4. Melhorar a documenta√ß√£o

## üìù Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes.

## üèÜ Resultados Finais

### Compara√ß√£o dos Algoritmos

| Algoritmo | Recompensa M√©dia | Desvio Padr√£o | Tempo de Avalia√ß√£o | Converg√™ncia |
|-----------|------------------|---------------|-------------------|--------------|
| PPO       | 185.3           | 45.2          | 0.18s            | 450k        |
| A2C       | 180.5           | 48.9          | 0.17s            | 400k        |
| DQN       | 175.8           | 52.7          | 0.20s            | 500k        |

### An√°lise dos Resultados

O PPO (Proximal Policy Optimization) demonstrou ser o algoritmo mais eficiente para o ambiente LunarLander, alcan√ßando a melhor recompensa m√©dia (185.3) e maior estabilidade (desvio padr√£o de 45.2). Sua converg√™ncia em 450k passos, combinada com um tempo de avalia√ß√£o competitivo de 0.18s, mostra um excelente equil√≠brio entre desempenho e efici√™ncia.

O A2C (Advantage Actor-Critic) apresentou um desempenho intermedi√°rio, com uma recompensa m√©dia de 180.5 e desvio padr√£o de 48.9. Sua principal vantagem foi a converg√™ncia mais r√°pida (400k passos) e o menor tempo de avalia√ß√£o (0.17s).

O DQN (Deep Q-Network) teve o desempenho mais modesto, com recompensa m√©dia de 175.8 e maior variabilidade (desvio padr√£o de 52.7). Sua converg√™ncia mais lenta (500k passos) e maior tempo de avalia√ß√£o (0.20s) indicam que pode n√£o ser a melhor escolha para este ambiente espec√≠fico.

### Compara√ß√£o com Outros Ambientes

| Caracter√≠stica | LunarLander | CartPole | Acrobot |
|----------------|-------------|----------|---------|
| Espa√ßo de Estados | 8 | 4 | 6 |
| Espa√ßo de A√ß√µes | 4 | 2 | 3 |
| Recompensa M√°xima | 200 | 500 | -100 |
| Complexidade | M√©dia | Baixa | M√©dia |
| Tempo de Treinamento | Maior | Menor | M√©dio |
| Estabilidade | Menor | Maior | M√©dia |
| Objetivo | Pousar | Equilibrar | Balan√ßar |
| Tipo de Recompensa | Mista | Positiva | Negativa |

### An√°lise Comparativa

1. **Complexidade do Ambiente**
   - LunarLander: Ambiente mais complexo dos tr√™s
   - Estados: Posi√ß√£o, velocidade, √¢ngulos, contato com o solo
   - A√ß√µes: Controle dos motores principais e laterais
   - Recompensa: Combina√ß√£o de pouso suave e consumo de combust√≠vel

2. **Desafios Espec√≠ficos**
   - Controle preciso dos motores
   - Gerenciamento de combust√≠vel
   - Pouso suave na √°rea alvo
   - Explora√ß√£o eficiente do espa√ßo de estados

3. **Ajustes Espec√≠ficos**
   - Learning rates mais conservadores
   - Maior √™nfase na explora√ß√£o
   - Buffer de replay maior para DQN
   - Mais √©pocas de treinamento para PPO

4. **Resultados Finais**
   - PPO: Melhor desempenho (185.3) e maior estabilidade (45.2)
   - A2C: Desempenho intermedi√°rio (180.5) e boa estabilidade (48.9)
   - DQN: Desempenho mais baixo (175.8) e menor estabilidade (52.7)
   - Todos os algoritmos convergiram antes dos 1M passos
   - Tempos de avalia√ß√£o similares (0.17-0.20s)
