# üöÄ Treinando Agentes no LunarLander-v2 com Aprendizado por Refor√ßo

Este reposit√≥rio cont√©m tr√™s scripts que treinam agentes de IA usando diferentes algoritmos de aprendizado por refor√ßo no ambiente **LunarLander-v2**. Os algoritmos utilizados s√£o:

- **DQN** (Deep Q-Network)
- **PPO** (Proximal Policy Optimization)
- **A2C** (Advantage Actor-Critic)

Abaixo, voc√™ encontrar√° uma explica√ß√£o sobre cada um dos m√©todos, o que cada script faz e como voc√™ pode executar e avaliar os modelos.

---

## üìÅ Estrutura do Reposit√≥rio

```
üìÇ root/
‚îÇ‚îÄ‚îÄ dqn_train_lunar.py     # Treina e avalia um modelo DQN
‚îÇ‚îÄ‚îÄ ppo_train_lunar.py     # Treina e avalia um modelo PPO
‚îÇ‚îÄ‚îÄ a2c_train_lunar.py     # Treina e avalia um modelo A2C
‚îÇ‚îÄ‚îÄ dqn_model_view.py      # Visualiza o comportamento do modelo DQN
‚îÇ‚îÄ‚îÄ ppo_model_view.py      # Visualiza o comportamento do modelo PPO
‚îÇ‚îÄ‚îÄ a2c_model_view.py      # Visualiza o comportamento do modelo A2C
```

---

## üìå Algoritmos Utilizados

### üîπ 1. DQN (Deep Q-Network)

> **DQN** √© um algoritmo de aprendizado por refor√ßo baseado em redes neurais profundas que aproxima a fun√ß√£o de valor Q. Ele √© eficaz para problemas com a√ß√µes discretas.

- **üìå Tipo:** Off-policy
- **üõ†Ô∏è Abordagem:** Valor de Q
- **‚úÖ Vantagens:** Ideal para ambientes com a√ß√µes discretas.
- **‚ùå Desvantagens:** Requer ajuste de hiperpar√¢metros e t√©cnicas como replay de experi√™ncia.

#### üèóÔ∏è O que o c√≥digo faz:
- O script **`dqn_train_lunar.py`** treina um modelo DQN no ambiente **LunarLander-v2** por **100.000 timesteps**.
- O modelo treinado √© salvo no arquivo **`dqn_lunarlander`**.
- O c√≥digo **`dqn_model_view.py`** carrega o modelo treinado e visualiza o desempenho do agente.

---

### üîπ 2. PPO (Proximal Policy Optimization)

> **PPO** √© um algoritmo baseado em gradiente de pol√≠tica que melhora a estabilidade do treinamento por meio de atualiza√ß√µes controladas.

- **üìå Tipo:** On-policy
- **üõ†Ô∏è Abordagem:** Pol√≠tica
- **‚úÖ Vantagens:** Est√°vel e f√°cil de implementar.
- **‚ùå Desvantagens:** Requer mais intera√ß√µes com o ambiente do que algoritmos off-policy.

#### üèóÔ∏è O que o c√≥digo faz:
- O script **`ppo_train_lunar.py`** treina um modelo PPO no **LunarLander-v2** por **100.000 timesteps**.
- O modelo treinado √© salvo no arquivo **`ppo_lunarlander`**.
- O c√≥digo **`ppo_model_view.py`** carrega o modelo treinado e permite visualizar o desempenho do agente.

---

### üîπ 3. A2C (Advantage Actor-Critic)

> **A2C** combina um ator (**policy network**) e um cr√≠tico (**value network**) para aprender uma pol√≠tica de a√ß√£o e estimar o valor do estado.

- **üìå Tipo:** On-policy
- **üõ†Ô∏è Abordagem:** Actor-Critic
- **‚úÖ Vantagens:** Atualiza√ß√£o eficiente da pol√≠tica e bom desempenho em ambientes cont√≠nuos.
- **‚ùå Desvantagens:** Sens√≠vel aos hiperpar√¢metros.

#### üèóÔ∏è O que o c√≥digo faz:
- O script **`a2c_train_lunar.py`** treina um modelo A2C no **LunarLander-v2** por **100.000 timesteps**.
- O modelo treinado √© salvo no arquivo **`a2c_lunarlander`**.
- O c√≥digo **`a2c_model_view.py`** carrega o modelo treinado e exibe o desempenho do agente.

---

## üìä Compara√ß√£o entre DQN, PPO e A2C

| Algoritmo  | Abordagem        | Tipo       | Vantagens  | Desvantagens  |
|------------|-----------------|------------|------------|---------------|
| **DQN**    | Valor de Q       | Off-policy | Ideal para a√ß√µes discretas | Requer replay de experi√™ncia |
| **PPO**    | Pol√≠tica         | On-policy  | Est√°vel e eficiente | Maior custo computacional |
| **A2C**    | Actor-Critic     | On-policy  | Atualiza√ß√£o r√°pida da pol√≠tica | Sens√≠vel a hiperpar√¢metros |

---
## üìà Avalia√ß√£o de Desempenho dos Modelos

Ap√≥s o treinamento, os modelos s√£o avaliados com base em **m√©tricas quantitativas e visuais**, permitindo uma compara√ß√£o aprofundada entre os algoritmos.

### üî¢ M√©tricas Quantitativas

- üéØ **Recompensa M√©dia por Epis√≥dio**  
  Avalia o desempenho geral do agente durante os epis√≥dios de teste.

- üìâ **Estabilidade do Desempenho**  
  Calculada pelo **desvio padr√£o** e pelo **coeficiente de varia√ß√£o** das recompensas. Um valor menor indica um comportamento mais consistente.

- ‚è±Ô∏è **N√∫mero de Epis√≥dios at√© a Converg√™ncia**  
  Mede aproximadamente quantos epis√≥dios foram necess√°rios at√© o agente atingir um desempenho consistente (ex: m√©dia ‚â• 200 em uma janela de 10 epis√≥dios).

- ‚öôÔ∏è **Tempo de Processamento**  
  Mede quanto tempo levou para avaliar o modelo completo (√∫til para comparar efici√™ncia computacional).

> Para calcular e visualizar essas m√©tricas, utilize o script:
**`python comparador.py`**

---

### üìä **Visualiza√ß√µes com Matplotlib**

O script comparador.py tamb√©m gera gr√°ficos usando a biblioteca matplotlib, que ajudam a interpretar os resultados de forma visual:

---

1Ô∏è‚É£ **Gr√°fico de Barras: Recompensa M√©dia com Desvio Padr√£o**  
Mostra a m√©dia de recompensas por algoritmo.  
As barras de erro indicam o desvio padr√£o.  
Ideal para comparar desempenho m√©dio e consist√™ncia.

```python
plt.bar(labels, media, yerr=desvio, capsize=8, color=colors)
plt.title("Recompensa M√©dia por Algoritmo (¬± Desvio Padr√£o)")
```

---

2Ô∏è‚É£ **Gr√°fico de Barras: Tempo de Execu√ß√£o**  
Mostra o tempo total de execu√ß√£o da avalia√ß√£o para cada modelo.  
√ötil para comparar a efici√™ncia computacional entre algoritmos.

```python
plt.bar(labels, tempo_execucao, color=colors)
plt.title("Tempo de Avalia√ß√£o dos Modelos")
```

---

3Ô∏è‚É£ **Boxplot: Distribui√ß√£o das Recompensas**  
Exibe a distribui√ß√£o das recompensas por epis√≥dio.  
Permite ver outliers, mediana e dispers√£o.  
√ötil para avaliar estabilidade e robustez dos modelos.

```python
plt.boxplot([dqn_rewards, ppo_rewards, a2c_rewards], labels=labels, patch_artist=True)
plt.title("Distribui√ß√£o das Recompensas por Epis√≥dio")
```

---

üé® **Estilo e Organiza√ß√£o**  
Usa `plt.style.use('ggplot')` para um visual moderno e claro.  
`plt.tight_layout()` √© usado para evitar sobreposi√ß√£o de elementos.  
As cores e t√≠tulos s√£o personalizados para clareza.

---
## ‚öôÔ∏è Como Rodar os C√≥digos

### 1Ô∏è‚É£ Instale as depend√™ncias:
```bash
pip install pygame
pip install gymnasium 
pip install stable-baselines3[extra]
```

### 2Ô∏è‚É£ Treine o modelo executando o c√≥digo de treinamento do algoritmo desejado:
```bash
# Para DQN:
python dqn_train_lunar.py

# Para PPO:
python ppo_train_lunar.py

# Para A2C:
python a2c_train_lunar.py
```

### 3Ô∏è‚É£ Visualize o modelo treinado executando o c√≥digo de visualiza√ß√£o:
```bash
# Para DQN:
python dqn_model_view.py

# Para PPO:
python ppo_model_view.py

# Para A2C:
python a2c_model_view.py
```

---

## üéØ Conclus√£o

Este reposit√≥rio oferece uma maneira pr√°tica de **treinar, salvar e visualizar** agentes de IA no **LunarLander-v2** usando diferentes algoritmos. Atrav√©s dos scripts fornecidos, voc√™ pode facilmente comparar o desempenho de **DQN, PPO e A2C**, bem como visualizar o comportamento dos agentes treinados em um ambiente gr√°fico.
