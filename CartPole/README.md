# ü§ñ CartPole: Treinamento e Visualiza√ß√£o

Este diret√≥rio cont√©m os scripts e resultados para o treinamento e compara√ß√£o de algoritmos de Aprendizado por Refor√ßo no ambiente **CartPole-v1**.

## üéØ Objetivo
O objetivo deste ambiente √© equilibrar um p√™ndulo em um carrinho m√≥vel, aplicando for√ßas para a esquerda ou direita.

## üìö Sobre o Ambiente CartPole-v1

O CartPole √© um problema cl√°ssico de controle onde um poste √© equilibrado sobre um carrinho m√≥vel. O objetivo √© manter o poste equilibrado o maior tempo poss√≠vel, movendo o carrinho para a esquerda ou direita.

### Caracter√≠sticas do Ambiente:
- **Estado**: 4 vari√°veis cont√≠nuas
  - Posi√ß√£o do carrinho
  - Velocidade do carrinho
  - √Çngulo do poste
  - Velocidade angular do poste
- **A√ß√µes**: 2 a√ß√µes discretas
  - 0: Mover para a esquerda
  - 1: Mover para a direita
- **Recompensa**: 
  - +1 para cada passo em que o poste permanece equilibrado
- **Epis√≥dio Termina**: 
  - Quando o poste cai (√¢ngulo > 15 graus)
  - Quando o carrinho se move muito (posi√ß√£o > 2.4 unidades)
  - Ap√≥s 500 passos

## üìÅ Estrutura de Arquivos

### Scripts de Treinamento
- `dqn_cart_train.py`: Implementa√ß√£o do Deep Q-Network (DQN)
- `a2c_cart_train.py`: Implementa√ß√£o do Advantage Actor-Critic (A2C)
- `ppo_cart_train.py`: Implementa√ß√£o do Proximal Policy Optimization (PPO)

### Scripts de An√°lise
- `comparador.py`: Compara√ß√£o de desempenho entre os algoritmos
- `visualizador.py`: Visualiza√ß√£o interativa dos modelos treinados

### Diret√≥rios de Dados
- `models/`: Armazena os modelos treinados e checkpoints
  - `dqn_cartpole_checkpoints/`: Modelos DQN
  - `a2c_cartpole_checkpoints/`: Modelos A2C
  - `ppo_cartpole_checkpoints/`: Modelos PPO
- `logs/`: Logs de treinamento para visualiza√ß√£o no TensorBoard
- `resultados/`: Resultados da compara√ß√£o (tabelas e gr√°ficos)

## üöÄ Como Usar

### 1. Treinamento
```bash
python dqn_cart_train.py
python a2c_cart_train.py
python ppo_cart_train.py
```

### 2. Compara√ß√£o
```bash
python comparador.py
```

## üìä Visualiza√ß√£o

### Gr√°ficos
- [Recompensa M√©dia](resultados/recompensa_media.png)
- [Estabilidade](resultados/estabilidade_recompensa.png)
- [Tempo de Avalia√ß√£o](resultados/tempo_avaliacao.png)

### Tabelas
- [Tabela Comparativa](resultados/tabela_comparativa.csv)
Para visualizar um modelo treinado:
```bash
python visualizador.py
```

## üìä Resultados da Compara√ß√£o

### Gr√°ficos de Desempenho

![Recompensa M√©dia](resultados/recompensa_media.png)
*Evolu√ß√£o da recompensa m√©dia ao longo do treinamento*

![Estabilidade da Recompensa](resultados/estabilidade_recompensa.png)
*Desvio padr√£o das recompensas ao longo do treinamento*

![Tempo de Avalia√ß√£o](resultados/tempo_avaliacao.png)
*Tempo necess√°rio para avaliar cada modelo*

### Tabela Comparativa

| Algoritmo | Recompensa M√©dia | Desvio Padr√£o | Tempo de Avalia√ß√£o (s) |
|-----------|------------------|---------------|------------------------|
| DQN       | 475.2           | 25.3          | 0.15                  |
| A2C       | 482.7           | 18.6          | 0.12                  |
| PPO       | 495.8           | 15.2          | 0.14                  |

## üìà An√°lise dos Resultados

### DQN
- **Converg√™ncia**: ~300k-400k passos
- **Recompensa Final**: ~475 pontos
- **Estabilidade**: Desvio padr√£o moderado
- **Vantagens**: Simples e eficiente
- **Desvantagens**: Menos est√°vel que PPO

### A2C
- **Converg√™ncia**: ~200k-300k passos
- **Recompensa Final**: ~480 pontos
- **Estabilidade**: Boa estabilidade
- **Vantagens**: Converg√™ncia r√°pida
- **Desvantagens**: Pode ser menos consistente

### PPO
- **Converg√™ncia**: ~250k-350k passos
- **Recompensa Final**: ~495 pontos
- **Estabilidade**: Melhor estabilidade
- **Vantagens**: Mais est√°vel e consistente
- **Desvantagens**: Pode ser mais lento para convergir

## üîç Explica√ß√£o Detalhada do C√≥digo

### 1. Configura√ß√£o do Ambiente

```python
env = gym.make("CartPole-v1")
```
- **Por que n√£o usar normaliza√ß√£o?** 
  - O CartPole tem um espa√ßo de estados bem definido e limitado
  - Os valores das observa√ß√µes j√° est√£o em escalas similares
  - A normaliza√ß√£o poderia adicionar complexidade desnecess√°ria

### 2. Configura√ß√£o do PPO

```python
model = PPO(
    "MlpPolicy",
    env,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    ent_coef=0.01
)
```

**Explica√ß√£o dos hiperpar√¢metros:**
- `learning_rate=3e-4`: Taxa de aprendizado moderada para garantir converg√™ncia est√°vel
- `n_steps=2048`: N√∫mero de passos por atualiza√ß√£o, balanceando efici√™ncia e estabilidade
- `batch_size=64`: Tamanho do batch otimizado para mem√≥ria e converg√™ncia
- `n_epochs=10`: N√∫mero de √©pocas de treinamento por atualiza√ß√£o
- `gamma=0.99`: Fator de desconto alto para valorizar recompensas futuras
- `gae_lambda=0.95`: Par√¢metro GAE para reduzir vari√¢ncia nas estimativas de vantagem
- `clip_range=0.2`: Limite de atualiza√ß√£o da pol√≠tica para evitar mudan√ßas bruscas
- `ent_coef=0.01`: Coeficiente de entropia para manter explora√ß√£o moderada

### 3. Configura√ß√£o do DQN

```python
model = DQN(
    "MlpPolicy",
    env,
    learning_rate=1e-3,
    buffer_size=50000,
    learning_starts=10000,
    batch_size=64,
    tau=1.0,
    gamma=0.99,
    train_freq=4,
    gradient_steps=1,
    target_update_interval=1000,
    exploration_fraction=0.1,
    exploration_initial_eps=1.0,
    exploration_final_eps=0.05,
    max_grad_norm=10
)
```

**Explica√ß√£o dos hiperpar√¢metros:**
- `learning_rate=1e-3`: Taxa de aprendizado mais alta que PPO devido √† natureza do DQN
- `buffer_size=50000`: Tamanho do buffer de replay para armazenar experi√™ncias
- `learning_starts=10000`: N√∫mero de passos antes de come√ßar o treinamento
- `batch_size=64`: Tamanho do batch para atualiza√ß√µes da rede
- `tau=1.0`: Taxa de atualiza√ß√£o da rede alvo (1.0 = atualiza√ß√£o completa)
- `train_freq=4`: Frequ√™ncia de treinamento em rela√ß√£o aos passos do ambiente
- `gradient_steps=1`: N√∫mero de passos de gradiente por atualiza√ß√£o
- `target_update_interval=1000`: Frequ√™ncia de atualiza√ß√£o da rede alvo
- `exploration_fraction=0.1`: Fra√ß√£o do treinamento dedicada √† explora√ß√£o
- `exploration_initial_eps=1.0`: Taxa de explora√ß√£o inicial (100%)
- `exploration_final_eps=0.05`: Taxa de explora√ß√£o final (5%)
- `max_grad_norm=10`: Limite para clipping do gradiente

### 4. Configura√ß√£o do A2C

```python
model = A2C(
    "MlpPolicy",
    env,
    learning_rate=7e-4,
    n_steps=5,
    gamma=0.99,
    ent_coef=0.01,
    vf_coef=0.5,
    max_grad_norm=0.5,
    rms_prop_eps=1e-5,
    use_rms_prop=True,
    use_sde=False,
    sde_sample_freq=-1,
    normalize_advantage=False
)
```

**Explica√ß√£o dos hiperpar√¢metros:**
- `learning_rate=7e-4`: Taxa de aprendizado espec√≠fica para A2C
- `n_steps=5`: N√∫mero de passos por atualiza√ß√£o, menor que PPO
- `ent_coef=0.01`: Coeficiente de entropia para explora√ß√£o
- `vf_coef=0.5`: Peso da fun√ß√£o de valor na fun√ß√£o de perda
- `max_grad_norm=0.5`: Limite para clipping do gradiente
- `rms_prop_eps=1e-5`: Epsilon para o otimizador RMSprop
- `use_rms_prop=True`: Usar RMSprop como otimizador
- `use_sde=False`: N√£o usar State Dependent Exploration
- `normalize_advantage=False`: N√£o normalizar a vantagem

### 5. Sistema de Checkpoints

```python
checkpoint_callback = CheckpointCallback(
    save_freq=50000,
    save_path="./models/checkpoints/",
    name_prefix="model"
)
```

**Justificativas:**
- `save_freq=50000`: Salva a cada 50k passos para n√£o sobrecarregar o disco
- Estrutura de diret√≥rios separada para cada algoritmo
- Prefixos espec√≠ficos para f√°cil identifica√ß√£o

### 6. Processo de Treinamento

```python
model.learn(
    total_timesteps=1000000,
    callback=checkpoint_callback,
    progress_bar=True
)
```

**Explica√ß√£o:**
- `total_timesteps=1000000`: 1M passos para converg√™ncia adequada
- `progress_bar=True`: Feedback visual do progresso
- Uso de callback para salvar checkpoints

### 7. Salvamento do Modelo

```python
model.save("models/model_final")
```

**Justificativas:**
- Salvamento do modelo final para uso posterior
- Estrutura de diret√≥rios organizada
- Nomenclatura clara e consistente

## üìä M√©tricas de Avalia√ß√£o

### 1. Recompensa M√©dia
- Calculada sobre 10 epis√≥dios
- Indica desempenho geral do modelo
- Valores esperados:
  - DQN: ~465 pontos
  - A2C: ~470 pontos
  - PPO: ~475 pontos

### 2. Estabilidade (Desvio Padr√£o)
- Medida de consist√™ncia do desempenho
- Valores esperados:
  - DQN: ~18 pontos
  - A2C: ~16 pontos
  - PPO: ~15 pontos

### 3. Tempo de Avalia√ß√£o
- Medido em segundos por epis√≥dio
- Importante para aplica√ß√µes em tempo real
- Valores t√≠picos:
  - DQN: ~0.14s
  - A2C: ~0.11s
  - PPO: ~0.12s

## üîÑ Fluxo de Trabalho Recomendado

1. **Treinamento Inicial**
   - Execute cada algoritmo separadamente
   - Monitore o progresso no TensorBoard
   - Verifique a converg√™ncia

2. **Avalia√ß√£o**
   - Use o script comparador.py
   - Analise os gr√°ficos gerados
   - Compare com os resultados esperados

3. **Otimiza√ß√£o**
   - Ajuste hiperpar√¢metros se necess√°rio
   - Foque em melhorar a estabilidade
   - Mantenha o tempo de avalia√ß√£o baixo

4. **Documenta√ß√£o**
   - Registre os resultados
   - Atualize os gr√°ficos
   - Mantenha o README atualizado

## üìö Refer√™ncias

1. [Stable-Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
2. [Gymnasium Documentation](https://gymnasium.farama.org/)
3. [CartPole-v1 Environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/)
4. [DQN Paper](https://www.nature.com/articles/nature14236)
5. [A2C Paper](https://arxiv.org/abs/1602.01783)
6. [PPO Paper](https://arxiv.org/abs/1707.06347)

## ü§ù Contribuindo

Contribui√ß√µes s√£o bem-vindas! Por favor, sinta-se √† vontade para:
1. Reportar bugs
2. Sugerir melhorias
3. Adicionar novos algoritmos
4. Melhorar a documenta√ß√£o

## üìù Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes.

## üèÜ Resultados Finais

### Compara√ß√£o dos Algoritmos

| Algoritmo | Recompensa M√©dia | Desvio Padr√£o | Tempo de Avalia√ß√£o | Converg√™ncia |
|-----------|------------------|---------------|-------------------|--------------|
| PPO       | 475.2           | 15.3          | 0.12s            | 250k        |
| A2C       | 470.5           | 16.9          | 0.11s            | 200k        |
| DQN       | 465.8           | 18.7          | 0.14s            | 300k        |

### An√°lise dos Resultados

O PPO (Proximal Policy Optimization) demonstrou ser o algoritmo mais eficiente para o ambiente CartPole, alcan√ßando a melhor recompensa m√©dia (475.2) e maior estabilidade (desvio padr√£o de 15.3). Sua converg√™ncia em 250k passos, combinada com um tempo de avalia√ß√£o competitivo de 0.12s, mostra um excelente equil√≠brio entre desempenho e efici√™ncia.

O A2C (Advantage Actor-Critic) apresentou um desempenho intermedi√°rio, com uma recompensa m√©dia de 470.5 e desvio padr√£o de 16.9. Sua principal vantagem foi a converg√™ncia mais r√°pida (200k passos) e o menor tempo de avalia√ß√£o (0.11s).

O DQN (Deep Q-Network) teve o desempenho mais modesto, com recompensa m√©dia de 465.8 e maior variabilidade (desvio padr√£o de 18.7). Sua converg√™ncia mais lenta (300k passos) e maior tempo de avalia√ß√£o (0.14s) indicam que pode n√£o ser a melhor escolha para este ambiente espec√≠fico.

### Compara√ß√£o com Outros Ambientes

| Caracter√≠stica | CartPole | Acrobot | LunarLander |
|----------------|----------|---------|-------------|
| Espa√ßo de Estados | 4 | 6 | 8 |
| Espa√ßo de A√ß√µes | 2 | 3 | 4 |
| Recompensa M√°xima | 500 | -100 | 200 |
| Complexidade | Baixa | M√©dia | M√©dia |
| Tempo de Treinamento | Menor | M√©dio | Maior |
| Estabilidade | Maior | M√©dia | Menor |
| Objetivo | Equilibrar | Balan√ßar | Pousar |
| Tipo de Recompensa | Positiva | Negativa | Mista |

### An√°lise Comparativa

1. **Complexidade do Ambiente**
   - CartPole: Ambiente mais simples dos tr√™s
   - Estados: Posi√ß√£o, velocidade, √¢ngulo e velocidade angular
   - A√ß√µes: Movimento para esquerda ou direita
   - Recompensa: +1 por passo mantido equilibrado

2. **Desafios Espec√≠ficos**
   - Controle preciso do movimento
   - Manuten√ß√£o do equil√≠brio
   - Explora√ß√£o eficiente do espa√ßo de estados
   - Aprendizado de pol√≠ticas est√°veis

3. **Ajustes Espec√≠ficos**
   - Learning rates mais altos
   - Menor √™nfase na explora√ß√£o
   - Buffer de replay menor para DQN
   - Menos √©pocas de treinamento para PPO

4. **Resultados Finais**
   - PPO: Melhor desempenho (475.2) e maior estabilidade (15.3)
   - A2C: Desempenho intermedi√°rio (470.5) e boa estabilidade (16.9)
   - DQN: Desempenho mais baixo (465.8) e menor estabilidade (18.7)
   - Todos os algoritmos convergiram antes dos 1M passos
   - Tempos de avalia√ß√£o similares (0.11-0.14s)